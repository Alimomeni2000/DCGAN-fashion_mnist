{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import (Conv2D,LeakyReLU,Dense,Flatten,\n",
        "                                     Dropout,BatchNormalization,UpSampling2D,\n",
        "                                     GlobalMaxPooling2D,Reshape,Conv2DTranspose)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import Model, callbacks\n",
        "from tensorflow.keras.preprocessing.image import array_to_img\n",
        "import os\n"
      ],
      "metadata": {
        "id": "qZvD91eRgjJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim= 128\n",
        "batch_size = 64\n",
        "(x_train,_),(x_test,_) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "all_images = np.concatenate((x_train,x_test), axis=0)\n",
        "all_images = all_images.astype(\"float32\") / 255.0\n",
        "all_images = np.reshape(all_images,(-1,28,28,1))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(all_images)\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McldE-b6qGUW",
        "outputId": "089b8a73-4d3b-40b5-e357-a6aa486889d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Create_Models():\n",
        "    def __init__(self,latent_dim):\n",
        "        \n",
        "        self.latent_size = latent_dim\n",
        "        \n",
        "    def build_disc(self):\n",
        "        model= Sequential()\n",
        "        model.add(Conv2D(32,3,padding='same', input_shape=(28,28,1)))\n",
        "        model.add(LeakyReLU(alpha=0.25))\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "        model.add(Conv2D(64,3,padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.25))\n",
        "        model.add(Dropout(0.5))\n",
        "         \n",
        "        model.add(Conv2D(128,3,strides=2,padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.25))\n",
        "        model.add(Dropout(0.5))\n",
        "        \n",
        "        model.add(Conv2D(256,3,strides=1,padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(LeakyReLU(alpha=0.25))\n",
        "        model.add(Flatten())\n",
        "        \n",
        "        model.add(Dropout(0.5))\n",
        "        \n",
        "#         model.add(GlobalMaxPooling2D())\n",
        "    \n",
        "        model.add(Dense(1, activation='sigmoid')) \n",
        "        \n",
        "        return model  \n",
        "    \n",
        "    def build_gen(self):\n",
        "        \n",
        "        model = tf.keras.models.Sequential()\n",
        "\n",
        "        model.add(tf.keras.layers.Dense(7*7*128,input_dim=self.latent_size))\n",
        "        model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
        "        model.add(tf.keras.layers.Reshape((7,7,128)))\n",
        "\n",
        "        #128 14*14\n",
        "        model.add(tf.keras.layers.Conv2DTranspose(128,4,strides=2,padding='same',kernel_initializer='glorot_normal'))\n",
        "        model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "        #128* 128*28*28\n",
        "        model.add(tf.keras.layers.Conv2DTranspose(128,4,strides=2,padding='same',kernel_initializer='glorot_normal'))\n",
        "        model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        \n",
        "        model.add(tf.keras.layers.Conv2DTranspose(64,2,strides=1,padding='same',kernel_initializer='glorot_normal'))\n",
        "        model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        \n",
        "        model.add(tf.keras.layers.Conv2D(1,7,padding='same',activation='tanh',kernel_initializer='glorot_normal'))\n",
        "\n",
        "        return model\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "OtyRemPWHXXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "creat_model = Create_Models(latent_dim)\n",
        "generator= creat_model.build_gen()\n",
        "discriminator= creat_model.build_disc()"
      ],
      "metadata": {
        "id": "YUzp-ZJLHZvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DCGAN(Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim,*args,**kwargs):\n",
        "        super(DCGAN, self).__init__(*args,**kwargs)\n",
        "        self.discriminator =discriminator\n",
        "        self.generator =generator        \n",
        "        self.latent_dim =latent_dim     \n",
        "        \n",
        "    def compileM(self,d_optimizer,g_optimizer, loss_fn,*args,**kwargs):\n",
        "        super(DCGAN,self).compile(*args,**kwargs)\n",
        "        self.d_optimizer =d_optimizer\n",
        "        self.g_optimizer =g_optimizer        \n",
        "        self.loss_fn =loss_fn     \n",
        "    \n",
        "    def train_step(self, real_images):\n",
        "        \n",
        "        batch_size= tf.shape(real_images)[0]\n",
        "        random_latent_vects = tf.random.normal(shape=(batch_size,self.latent_dim))\n",
        "        gen_images = self.generator(random_latent_vects)\n",
        "        combind_images= tf.concat([gen_images, real_images],axis=0)\n",
        "        \n",
        "        \n",
        "        # train discriminator\n",
        "        \n",
        "        labels = tf.concat([tf.ones((batch_size,1)), tf.zeros((batch_size,1))],axis=0)\n",
        "        labels += 0.04* tf.random.uniform(tf.shape(labels))\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combind_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "            \n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "         \n",
        "        # train generator\n",
        "        \n",
        "        random_latent_vects = tf.random.normal(shape=(batch_size,self.latent_dim))\n",
        "        fake_labels = tf.zeros((batch_size, 1))\n",
        "                \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vects))\n",
        "            g_loss = self.loss_fn(fake_labels,predictions)\n",
        "            \n",
        "\n",
        "        grads = tape.gradient(g_loss,self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(\n",
        "            zip(grads,self.generator.trainable_weights)\n",
        "        )\n",
        "        return {\"d_loss\":d_loss,\"g_loss\":g_loss}\n"
      ],
      "metadata": {
        "id": "0r0it7CaHdjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DCGAN_Monitor(callbacks.Callback):\n",
        "    def __init__(self,latent_dim,num_image=3):\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_image= num_image\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vects =tf.random.normal(shape=(16, self.latent_dim))\n",
        "        gen_images = self.model.generator(random_latent_vects)\n",
        "        generator_images = gen_images.numpy()\n",
        "        plt.close('all')\n",
        "        fig,axs = plt.subplots(4,4,figsize=(4,4),sharey=True,sharex=True)\n",
        "        for i in range(self.num_image):\n",
        "          img= array_to_img(generator_images[i])\n",
        "\n",
        "        c=0\n",
        "        for i in range(4):\n",
        "            for j in range(4):\n",
        "                axs[i,j].imshow(gen_images[c,:,:,0],cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                c+=1\n",
        "\n",
        "        fig.show()\n",
        "                "
      ],
      "metadata": {
        "id": "NNcwyMDaHfoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dcgan = DCGAN(discriminator=discriminator,generator=generator,latent_dim=latent_dim)\n",
        "dcgan.compileM(\n",
        "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001,beta_1=0.5),\n",
        "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001,beta_1=0.5),\n",
        "    loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        ")\n",
        "dcgan.fit(dataset,epochs= 200 ,callbacks=[DCGAN_Monitor(latent_dim=latent_dim)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGTwyBDKHh1z",
        "outputId": "2ef69c07-4ff6-4d30-cc2b-ddde5a7e869a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5676: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1094/1094 [==============================] - 84s 62ms/step - d_loss: 0.2714 - g_loss: 2.4431\n",
            "Epoch 2/200\n",
            "1094/1094 [==============================] - 67s 61ms/step - d_loss: 0.1780 - g_loss: 3.1339\n",
            "Epoch 3/200\n",
            "1094/1094 [==============================] - 68s 62ms/step - d_loss: 0.2243 - g_loss: 2.8309\n",
            "Epoch 4/200\n",
            "1094/1094 [==============================] - 66s 61ms/step - d_loss: 0.2518 - g_loss: 2.4634\n",
            "Epoch 5/200\n",
            "1094/1094 [==============================] - 66s 61ms/step - d_loss: 0.2259 - g_loss: 2.5767\n",
            "Epoch 6/200\n",
            "1094/1094 [==============================] - 67s 61ms/step - d_loss: 0.2280 - g_loss: 2.6991\n",
            "Epoch 7/200\n",
            "1094/1094 [==============================] - 67s 61ms/step - d_loss: 0.2233 - g_loss: 2.7248\n",
            "Epoch 8/200\n",
            "1094/1094 [==============================] - 66s 61ms/step - d_loss: 0.2262 - g_loss: 2.7175\n",
            "Epoch 9/200\n",
            "1094/1094 [==============================] - 66s 61ms/step - d_loss: 0.2310 - g_loss: 2.6932\n",
            "Epoch 10/200\n",
            "1094/1094 [==============================] - 66s 61ms/step - d_loss: 0.2326 - g_loss: 2.6949\n",
            "Epoch 11/200\n",
            "1094/1094 [==============================] - 67s 61ms/step - d_loss: 0.2449 - g_loss: 2.6608\n",
            "Epoch 12/200\n",
            "1094/1094 [==============================] - 67s 61ms/step - d_loss: 0.2490 - g_loss: 2.6171\n",
            "Epoch 13/200\n",
            "1094/1094 [==============================] - 67s 61ms/step - d_loss: 0.2523 - g_loss: 2.5956\n",
            "Epoch 14/200\n",
            "1094/1094 [==============================] - 67s 61ms/step - d_loss: 0.2582 - g_loss: 2.5569\n",
            "Epoch 15/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.2643 - g_loss: 2.5089\n",
            "Epoch 16/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.2651 - g_loss: 2.4720\n",
            "Epoch 17/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.2740 - g_loss: 2.4487\n",
            "Epoch 18/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.2728 - g_loss: 2.4268\n",
            "Epoch 19/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.2801 - g_loss: 2.4104\n",
            "Epoch 20/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.2845 - g_loss: 2.3663\n",
            "Epoch 21/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.2891 - g_loss: 2.3542\n",
            "Epoch 22/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.2910 - g_loss: 2.3300\n",
            "Epoch 23/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.2983 - g_loss: 2.2951\n",
            "Epoch 24/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3063 - g_loss: 2.2559\n",
            "Epoch 25/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3067 - g_loss: 2.2420\n",
            "Epoch 26/200\n",
            "1094/1094 [==============================] - 65s 59ms/step - d_loss: 0.3103 - g_loss: 2.2149\n",
            "Epoch 27/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3155 - g_loss: 2.2103\n",
            "Epoch 28/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3149 - g_loss: 2.1727\n",
            "Epoch 29/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3226 - g_loss: 2.1722\n",
            "Epoch 30/200\n",
            "1094/1094 [==============================] - 65s 59ms/step - d_loss: 0.3262 - g_loss: 2.1521\n",
            "Epoch 31/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3275 - g_loss: 2.1233\n",
            "Epoch 32/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3324 - g_loss: 2.1098\n",
            "Epoch 33/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3367 - g_loss: 2.0969\n",
            "Epoch 34/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3348 - g_loss: 2.0879\n",
            "Epoch 35/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3399 - g_loss: 2.0716\n",
            "Epoch 36/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3386 - g_loss: 2.0457\n",
            "Epoch 37/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3439 - g_loss: 2.0628\n",
            "Epoch 38/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3468 - g_loss: 2.0370\n",
            "Epoch 39/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3450 - g_loss: 2.0210\n",
            "Epoch 40/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3462 - g_loss: 2.0375\n",
            "Epoch 41/200\n",
            "1094/1094 [==============================] - 65s 59ms/step - d_loss: 0.3448 - g_loss: 2.0287\n",
            "Epoch 42/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3477 - g_loss: 2.0347\n",
            "Epoch 43/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3484 - g_loss: 2.0168\n",
            "Epoch 44/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3463 - g_loss: 2.0387\n",
            "Epoch 45/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3510 - g_loss: 2.0267\n",
            "Epoch 46/200\n",
            "1094/1094 [==============================] - 66s 61ms/step - d_loss: 0.3517 - g_loss: 2.0756\n",
            "Epoch 47/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3493 - g_loss: 2.0046\n",
            "Epoch 48/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3529 - g_loss: 2.0110\n",
            "Epoch 49/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3539 - g_loss: 1.9891\n",
            "Epoch 50/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3517 - g_loss: 2.0076\n",
            "Epoch 51/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3551 - g_loss: 1.9886\n",
            "Epoch 52/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3531 - g_loss: 2.0113\n",
            "Epoch 53/200\n",
            "1094/1094 [==============================] - 66s 61ms/step - d_loss: 0.3539 - g_loss: 1.9989\n",
            "Epoch 54/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3595 - g_loss: 1.9826\n",
            "Epoch 55/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3547 - g_loss: 1.9877\n",
            "Epoch 56/200\n",
            "1094/1094 [==============================] - 66s 61ms/step - d_loss: 0.3619 - g_loss: 1.9739\n",
            "Epoch 57/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3534 - g_loss: 1.9933\n",
            "Epoch 58/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3520 - g_loss: 2.0074\n",
            "Epoch 59/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3531 - g_loss: 2.0038\n",
            "Epoch 60/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3561 - g_loss: 1.9971\n",
            "Epoch 61/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3592 - g_loss: 1.9829\n",
            "Epoch 62/200\n",
            "1094/1094 [==============================] - 65s 59ms/step - d_loss: 0.3624 - g_loss: 1.9703\n",
            "Epoch 63/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3469 - g_loss: 2.0388\n",
            "Epoch 64/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3517 - g_loss: 2.0103\n",
            "Epoch 65/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3534 - g_loss: 2.0390\n",
            "Epoch 66/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3487 - g_loss: 2.0182\n",
            "Epoch 67/200\n",
            "1094/1094 [==============================] - 66s 61ms/step - d_loss: 0.3448 - g_loss: 2.0395\n",
            "Epoch 68/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3509 - g_loss: 2.0349\n",
            "Epoch 69/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3458 - g_loss: 2.0616\n",
            "Epoch 70/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3498 - g_loss: 2.0314\n",
            "Epoch 71/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3513 - g_loss: 2.0312\n",
            "Epoch 72/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3527 - g_loss: 2.0440\n",
            "Epoch 73/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3476 - g_loss: 2.0633\n",
            "Epoch 74/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3413 - g_loss: 2.0693\n",
            "Epoch 75/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3462 - g_loss: 2.0640\n",
            "Epoch 76/200\n",
            "1094/1094 [==============================] - 65s 59ms/step - d_loss: 0.3274 - g_loss: 2.5064\n",
            "Epoch 77/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3406 - g_loss: 2.0774\n",
            "Epoch 78/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3389 - g_loss: 2.0940\n",
            "Epoch 79/200\n",
            "1094/1094 [==============================] - 67s 61ms/step - d_loss: 0.3405 - g_loss: 2.0951\n",
            "Epoch 80/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3419 - g_loss: 2.0845\n",
            "Epoch 81/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3358 - g_loss: 2.1176\n",
            "Epoch 82/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3367 - g_loss: 2.1154\n",
            "Epoch 83/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3462 - g_loss: 2.0810\n",
            "Epoch 84/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3476 - g_loss: 2.0997\n",
            "Epoch 85/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3400 - g_loss: 2.1163\n",
            "Epoch 86/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3367 - g_loss: 2.1284\n",
            "Epoch 87/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3295 - g_loss: 2.1437\n",
            "Epoch 88/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3063 - g_loss: 2.6470\n",
            "Epoch 89/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3349 - g_loss: 2.1283\n",
            "Epoch 90/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3344 - g_loss: 2.1531\n",
            "Epoch 91/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3310 - g_loss: 2.1581\n",
            "Epoch 92/200\n",
            "1094/1094 [==============================] - 65s 60ms/step - d_loss: 0.3323 - g_loss: 2.1633\n",
            "Epoch 93/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3249 - g_loss: 2.1746\n",
            "Epoch 94/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3285 - g_loss: 2.1730\n",
            "Epoch 95/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3318 - g_loss: 2.1784\n",
            "Epoch 96/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3232 - g_loss: 2.1993\n",
            "Epoch 97/200\n",
            "1094/1094 [==============================] - 66s 60ms/step - d_loss: 0.3251 - g_loss: 2.2078\n",
            "Epoch 98/200\n",
            " 915/1094 [========================>.....] - ETA: 10s - d_loss: 0.3259 - g_loss: 2.2129"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = generator.predict(tf.random.normal((16,128,1)))\n",
        "plt.close('all')\n",
        "fig,axs = plt.subplots(4,4,figsize=(4,4),sharey=True,sharex=True)\n",
        "\n",
        "c=0\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        axs[i,j].imshow(imgs[(i+1)*(j+1)-1],cmap='gray')\n",
        "        axs[i,j].axis('off')\n",
        "        c+=1\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ddfAW0bRPNyf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}